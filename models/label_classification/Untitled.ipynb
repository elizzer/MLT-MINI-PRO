{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3808d73-06ef-464d-8bd7-3cd63922a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19ba70d9-35ef-417e-ad64-f950677f4805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>span</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great post !\\r\\nJustin at the 150 apologizing...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mark Hume has always been the stereotypical Va...</td>\n",
       "      <td>[149, 150, 151, 152, 153, 154, 155, 156, 157, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Freakin maggots, I swear.</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>so far the stupidest comments are about the NR...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‘Stupidity is a more dangerous enemy of the go...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  \\\n",
       "0   Great post !\\r\\nJustin at the 150 apologizing...   \n",
       "1  Mark Hume has always been the stereotypical Va...   \n",
       "2                          Freakin maggots, I swear.   \n",
       "3  so far the stupidest comments are about the NR...   \n",
       "4  ‘Stupidity is a more dangerous enemy of the go...   \n",
       "\n",
       "                                                span  label  \n",
       "0                                                 []      0  \n",
       "1  [149, 150, 151, 152, 153, 154, 155, 156, 157, ...      1  \n",
       "2                                                 []      1  \n",
       "3                                                 []      1  \n",
       "4                                                 []      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"../../datasets/dataset_removed_repeated_charcter.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7fa76b1-c599-45db-b78d-3ea0d752eee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68ab840a-2660-494e-a780-8794409a0835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         Great post !\\r\\nJustin at the 150 apologizing...\n",
      "1        Mark Hume has always been the stereotypical Va...\n",
      "2                                Freakin maggots, I swear.\n",
      "3        so far the stupidest comments are about the NR...\n",
      "4        ‘Stupidity is a more dangerous enemy of the go...\n",
      "                               ...                        \n",
      "20466    He might have been sleazy but trump is both sl...\n",
      "20467    Seems like the single payer system would be be...\n",
      "20468                                               Idiot!\n",
      "20469    My only Question is:  3 staff attorneys chosen...\n",
      "20470    \"They look like fools\", ignorance must be blis...\n",
      "Name: comment_text, Length: 20471, dtype: object\n",
      "0        0\n",
      "1        1\n",
      "2        1\n",
      "3        1\n",
      "4        0\n",
      "        ..\n",
      "20466    0\n",
      "20467    0\n",
      "20468    1\n",
      "20469    0\n",
      "20470    1\n",
      "Name: label, Length: 20471, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X=df[\"comment_text\"]\n",
    "Y=df[\"label\"]\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29905d66-9aea-4d6f-ba26-a77d09ad560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b47c399-998b-4a98-b60b-a5d447f111dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer,AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e5fc552-9851-4283-a801-4c95ec4c5f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "MAX_LEN=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bae931ef-2a18-421b-a1ec-0a10a3d8a55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output=tokenizer.encode_plus(\"Hello all\",max_length=MAX_LEN,padding='max_length',\n",
    "                return_token_type_ids=True,\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt')\n",
    "output\n",
    "output['input_ids'].flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7e60df-a39b-473d-b0ce-5c02b8602acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=tokenizer(\"Hello all\",max_length=MAX_LEN,padding='max_length')\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8e92315-eaad-4357-a12b-1c27b017ba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, df, tokenizer, max_len):\n",
    "            self.df = df\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_len= max_len\n",
    "            self.title=X\n",
    "            self.targets=Y\n",
    "        def __len__(self):\n",
    "            return len(self.title)\n",
    "\n",
    "        def __getitem__(self,index):\n",
    "            title = str(self.title[index])\n",
    "            title = \" \".join(title.split())\n",
    "\n",
    "            inputs = self.tokenizer(\n",
    "                title,None,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len,\n",
    "                padding='max_length',\n",
    "                return_token_type_ids=True,\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            # print(f\"[+]Index ok {index}\",inputs['input_ids'].flatten().shape)\n",
    "            # print(f\"[^]Have text {title}\")\n",
    "            # return {\n",
    "            #     'input_ids': inputs['input_ids'].flatten(),\n",
    "            #     'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            #     'token_type_ids': inputs['token_type_ids'].flatten(),\n",
    "            #     'targets': torch.FloatTensor(self.targets[index])\n",
    "            # }\n",
    "            target=self.targets[index]\n",
    "            return inputs,target\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "488b65f7-c194-4b15-9604-54ea9fb42870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 message  label\n",
      "56     These bums ripped off the US for many tens of ...      1\n",
      "15707  Why is the G&M giving KOL air-time?? \\r\\nThis ...      1\n",
      "808    Putin must be wetting his pants with excitemen...      0\n",
      "18810  These idiots disgust me! Just because they sho...      1\n",
      "4637   You would think. But then you are looking at a...      0\n",
      "                                                 message  label\n",
      "16208                                   We elected them.      0\n",
      "8169   Giving a star to your own comment. How patheti...      1\n",
      "4961   And there we go. Another attack on Canadians' ...      1\n",
      "8428   \". the angry populist movement that lashes out...      1\n",
      "2505   Please move to Canada since you only care abou...      1\n"
     ]
    }
   ],
   "source": [
    "train_size=0.8\n",
    "train_df={}\n",
    "train_df[\"message\"]=X_train\n",
    "train_df[\"label\"]=Y_train\n",
    "\n",
    "train_df=pd.DataFrame(train_df)\n",
    "\n",
    "\n",
    "val_df={}\n",
    "\n",
    "val_df[\"message\"]=X_test\n",
    "val_df[\"label\"]=Y_test\n",
    "val_df=pd.DataFrame(val_df)\n",
    "\n",
    "print(train_df.head())\n",
    "print(val_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78bc61c4-09ce-4a6a-8b45-6383e60e6f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=CustomDataset(train_df,tokenizer,MAX_LEN)\n",
    "val_dataset=CustomDataset(val_df,tokenizer,MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "711945c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader=torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    # shuffle=True,\n",
    "    batch_size=64,\n",
    "    num_workers=0\n",
    "\n",
    ")\n",
    "val_data_loader=torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    # shuffle=True,\n",
    "    batch_size=64,\n",
    "    num_workers=0\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd013ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device= torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f5aedcf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToxicClassiferModel(\n",
       "  (BERTLayer): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropoutLayer): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ToxicClassiferModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.BERTLayer=AutoModel.from_pretrained(\"bert-base-uncased\",return_dict=True)\n",
    "        self.dropoutLayer=nn.Dropout(0.3)\n",
    "        self.fc=nn.Linear(768,1)\n",
    "    \n",
    "    def forward(self,input_ids,attention_mask,token_type_id):\n",
    "        print(\"[+]Forward\")\n",
    "        output=self.BERTLayer(input_ids,attention_mask,token_type_id,return_dict=True)\n",
    "        output=self.dropoutLayer(output.pooler_output)\n",
    "        output=self.fc(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "model=ToxicClassiferModel()\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d74ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs,target):\n",
    "    return nn.BCEWithLogitsLoss()(outputs,target)\n",
    "\n",
    "optimizer=torch.optim.Adam(params=model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106a6ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    val_loss=0\n",
    "    model.train()\n",
    "\n",
    "    for index,(inputs,target) in enumerate(train_data_loader):\n",
    "    \n",
    "        # print(f'[+]Batch {index}')\n",
    "        input_ids=inputs['input_ids'].view(64,512).to(device)\n",
    "        attention_mask=inputs['attention_mask'].view(64,512).to(device)\n",
    "        token_type_ids=inputs['token_type_ids'].view(64,512).to(device)\n",
    "        print(input_ids.shape)\n",
    "        print(attention_mask.shape)\n",
    "        print(token_type_ids.shape)\n",
    "        target=target.to(device)\n",
    "        output=model(input_ids,attention_mask,token_type_ids)\n",
    "        print(\"[+]Output\",output[0])\n",
    "        break\n",
    "    print(f'[+]Epoch {epoch}')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e533ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697eae0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
