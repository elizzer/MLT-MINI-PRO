# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16kB9_3944OyorVhJ7uMOmxlhMvC3y9kb
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install transformers
# %pip install datasets

import tensorflow as tf

# Set memory growth before initializing any physical devices
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)

import tensorflow as tf
from transformers import TFAutoModel, AutoTokenizer
from datasets import load_dataset
from datasets import DatasetDict, Dataset

import pandas as pd

model = TFAutoModel.from_pretrained("bert-base-uncased")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

from google.colab import drive
drive.mount('/content/drive')

# dataset = load_dataset('csv', data_files=['/content/drive/MyDrive/MiniProject/splitDataset/train.csv'], delimiter=",")

train_dataset=Dataset.from_pandas(pd.read_csv('/content/drive/MyDrive/MiniProject/splitDataset/train.csv'))
test_dataset=Dataset.from_pandas(pd.read_csv('/content/drive/MyDrive/MiniProject/splitDataset/test.csv'))
valid_dataset=Dataset.from_pandas(pd.read_csv('/content/drive/MyDrive/MiniProject/splitDataset/valid.csv'))

dataset = DatasetDict({
    "train": train_dataset,
    "test": test_dataset,
    "valid": valid_dataset,
})
dataset

def tokenize(batch):
    return tokenizer(batch["comment_text"], padding=True, truncation=True)

text_encoded=dataset.map(tokenize, batched=True, batch_size=None)

text_encoded.set_format('tf',columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])

BATCH_SIZE = 24

def order(inp):
    '''
    This function will group all the inputs of BERT
    into a single dictionary and then output it with
    labels.
    '''
    data = list(inp.values())
    return {
        'input_ids': data[1],
        'attention_mask': data[2],
        'token_type_ids': data[3]
    }, data[0]

train_dataset = tf.data.Dataset.from_tensor_slices(text_encoded['train'][:])
# set batch_size and shuffle
train_dataset = train_dataset.batch(BATCH_SIZE).shuffle(1000)
# map the `order` function
train_dataset = train_dataset.map(order, num_parallel_calls=tf.data.AUTOTUNE)

# ... doing the same for test set ...
test_dataset = tf.data.Dataset.from_tensor_slices(text_encoded['test'][:])
test_dataset = test_dataset.batch(BATCH_SIZE)
test_dataset = test_dataset.map(order, num_parallel_calls=tf.data.AUTOTUNE)

valid_dataset = tf.data.Dataset.from_tensor_slices(text_encoded['valid'][:])
valid_dataset = valid_dataset.batch(BATCH_SIZE)
valid_dataset = valid_dataset.map(order, num_parallel_calls=tf.data.AUTOTUNE)

class BERTForClassification(tf.keras.Model):

    def __init__(self, bert_model, num_classes):
        super().__init__()
        self.bert = bert_model
        self.fc = tf.keras.layers.Dense(num_classes, activation='softmax')
        self.dropout = tf.keras.layers.Dropout(0.1)

    def call(self, inputs):
        x = self.bert(inputs)[1]
        x = self.dropout(x)
        return self.fc(x)

classifier = BERTForClassification(model, num_classes=1)

classifier.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),
    loss=tf.keras.losses.BinaryCrossentropy(),
    metrics=['accuracy']
)

history = classifier.fit(
    train_dataset,
    epochs=5,
    validation_data=valid_dataset

)

